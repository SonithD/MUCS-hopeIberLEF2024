{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH-DUyD6pU5E",
        "outputId": "dac9f247-dfac-4949-fb7b-5f950848a37d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\user\\anaconda3\\lib\\site-packages (2.13.0)\n",
            "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.25.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (68.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow)\n",
            "  Obtaining dependency information for typing-extensions<4.6.0,>=3.6.6 from https://files.pythonhosted.org/packages/31/25/5abcd82372d3d4a3932e1fa8c3dbf9efac10cc7c0d16e78467460571b404/typing_extensions-4.5.0-py3-none-any.whl.metadata\n",
            "  Using cached typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.57.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.22.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: urllib3<2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
            "Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: typing-extensions\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "Successfully installed typing-extensions-4.5.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydantic 2.6.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.16.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "torch 2.2.0 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typeguard 4.2.1 requires typing-extensions>=4.10.0; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "ERROR: Could not find a version that satisfies the requirement tensorflow-text (from versions: none)\n",
            "ERROR: No matching distribution found for tensorflow-text\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\user\\anaconda3\\lib\\site-packages (3.7.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.2)\n",
            "Collecting typing-extensions>=4.6.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Obtaining dependency information for typing-extensions>=4.6.1 from https://files.pythonhosted.org/packages/01/f3/936e209267d6ef7510322191003885de524fc48d1b43269810cd589ceaf5/typing_extensions-4.11.0-py3-none-any.whl.metadata\n",
            "  Using cached typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
            "Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: typing-extensions\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "Successfully installed typing-extensions-4.11.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.11.0 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in c:\\users\\user\\anaconda3\\lib\\site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in c:\\users\\user\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in c:\\users\\user\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
            "Requirement already satisfied: emoji in c:\\users\\user\\anaconda3\\lib\\site-packages (2.10.1)\n",
            "Requirement already satisfied: emot in c:\\users\\user\\anaconda3\\lib\\site-packages (3.1)\n",
            "Requirement already satisfied: demoji in c:\\users\\user\\anaconda3\\lib\\site-packages (1.1.0)\n",
            "Requirement already satisfied: nlpaug in c:\\users\\user\\anaconda3\\lib\\site-packages (1.1.11)\n",
            "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nlpaug) (1.24.3)\n",
            "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nlpaug) (2.31.0)\n",
            "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nlpaug) (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.9.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2022.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm->gdown>=4.0.0->nlpaug) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install tensorflow-text\n",
        "!pip install spacy\n",
        "!pip install contractions\n",
        "!pip install emoji\n",
        "!pip install emot\n",
        "!pip install demoji\n",
        "!pip install nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlrqbU2YrfM6",
        "outputId": "e0e69be4-4859-4176-cee6-e70b2c6edb2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.30.2\n",
            "  Obtaining dependency information for transformers==4.30.2 from https://files.pythonhosted.org/packages/5b/0b/e45d26ccd28568013523e04f325432ea88a442b4e3020b757cf4361f0120/transformers-4.30.2-py3-none-any.whl.metadata\n",
            "  Using cached transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers==4.30.2) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers==4.30.2) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers==4.30.2) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers==4.30.2) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers==4.30.2) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers==4.30.2) (2022.7.9)\n",
            "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers==4.30.2) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2)\n",
            "  Obtaining dependency information for tokenizers!=0.11.3,<0.14,>=0.11.1 from https://files.pythonhosted.org/packages/62/41/93d3135ec30f596a71490ce11a73572190fe80e85a2aea18f116a520cc41/tokenizers-0.13.3-cp311-cp311-win_amd64.whl.metadata\n",
            "  Using cached tokenizers-0.13.3-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers==4.30.2) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers==4.30.2) (4.65.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.11.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.30.2) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers==4.30.2) (2024.2.2)\n",
            "Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "Using cached tokenizers-0.13.3-cp311-cp311-win_amd64.whl (3.5 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.39.3\n",
            "    Uninstalling transformers-4.39.3:\n",
            "      Successfully uninstalled transformers-4.39.3\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "simpletransformers 0.65.1 requires transformers>=4.31.0, but you have transformers 4.30.2 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: simpletransformers in c:\\users\\user\\anaconda3\\lib\\site-packages (0.65.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (4.65.0)\n",
            "Requirement already satisfied: regex in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (2022.7.9)\n",
            "Collecting transformers>=4.31.0 (from simpletransformers)\n",
            "  Obtaining dependency information for transformers>=4.31.0 from https://files.pythonhosted.org/packages/15/fc/7b6dd7e1adc0a6407b845ed4be1999e98b6917d0694e57316d140cc85484/transformers-4.39.3-py3-none-any.whl.metadata\n",
            "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "Requirement already satisfied: datasets in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (2.16.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (1.3.0)\n",
            "Requirement already satisfied: seqeval in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: tensorboard in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (2.13.0)\n",
            "Requirement already satisfied: tensorboardx in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (2.6.2.2)\n",
            "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (1.5.3)\n",
            "Requirement already satisfied: tokenizers in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (0.13.3)\n",
            "Requirement already satisfied: wandb>=0.10.32 in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (0.16.3)\n",
            "Requirement already satisfied: streamlit in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (1.31.0)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\user\\anaconda3\\lib\\site-packages (from simpletransformers) (0.1.99)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.47.0->simpletransformers) (0.4.6)\n",
            "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (6.0)\n",
            "Collecting tokenizers (from simpletransformers)\n",
            "  Obtaining dependency information for tokenizers from https://files.pythonhosted.org/packages/c1/02/40725eebedea8175918bd59ab80b2174d6ef3b3ef9ac8ec996e84c38d3ca/tokenizers-0.15.2-cp311-none-win_amd64.whl.metadata\n",
            "  Using cached tokenizers-0.15.2-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (0.4.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (8.0.4)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (3.1.41)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (5.9.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.40.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in c:\\users\\user\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.3.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (68.0.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (4.25.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->simpletransformers) (2024.2.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (11.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (0.3.7)\n",
            "Requirement already satisfied: xxhash in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (3.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->simpletransformers) (2022.7)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->simpletransformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->simpletransformers) (2.2.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (5.2.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (1.7.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (5.3.1)\n",
            "Requirement already satisfied: importlib-metadata<8,>=1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (6.0.0)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (9.4.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (8.2.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (4.11.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (5.2)\n",
            "Requirement already satisfied: validators<1,>=0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.22.0)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.8.1b0)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (6.3.2)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (2.1.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (2.22.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.38.4)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.17.3)\n",
            "Requirement already satisfied: toolz in c:\\users\\user\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.0)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb>=0.10.32->simpletransformers) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-metadata<8,>=1.4->streamlit->simpletransformers) (3.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.15.1)\n",
            "Requirement already satisfied: tzdata in c:\\users\\user\\anaconda3\\lib\\site-packages (from tzlocal<6,>=1.1->streamlit->simpletransformers) (2023.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (3.2.2)\n",
            "Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "Using cached tokenizers-0.15.2-cp311-none-win_amd64.whl (2.2 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.30.2\n",
            "    Uninstalling transformers-4.30.2:\n",
            "      Successfully uninstalled transformers-4.30.2\n",
            "Successfully installed tokenizers-0.15.2 transformers-4.39.3\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.30.2\n",
        "!pip install simpletransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxxVhV94peyB",
        "outputId": "78575cdc-2642-497c-a26e-0ca112cb52bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12104\\183952326.py:24: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
            "  demoji.download_codes()\n"
          ]
        }
      ],
      "source": [
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import contractions\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import regex\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "import demoji\n",
        "import emoji\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "# os.environ[\"MODEL_DIR\"] = '/content/drive/MyDrive/homo/Model'\n",
        "# import nlpaug.augmenter.char as nac\n",
        "# import nlpaug.augmenter.word as naw\n",
        "from nlpaug.util import Action\n",
        "demoji.download_codes()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "from sklearn.svm import LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p17NXW_pldt",
        "outputId": "4861f4c9-3b57-4d94-8a78-6f8f12a0f448"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading whitespace: Package 'whitespace' not found\n",
            "[nltk_data]     in index\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('whitespace')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "stop_words = set(stopwords.words(\"english\",\"spanish\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cil9ooIpqgX"
      },
      "outputs": [],
      "source": [
        "df_train_Eng = pd.read_csv('Task 2 Training set (English).csv')\n",
        "df_val_Eng=pd.read_csv('val.csv')\n",
        "df_train_span=pd.read_csv('train.csv')\n",
        "df_val_span=pd.read_csv('val (1).csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx6DAW1a51SM"
      },
      "outputs": [],
      "source": [
        "df_test_Eng=pd.read_csv('test - no_label.csv')\n",
        "df_test_span=pd.read_csv('Task2_ES_test_no_label.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpXydsiA51SN"
      },
      "outputs": [],
      "source": [
        "frame1=(df_train_Eng,df_train_span)\n",
        "combined_train=pd.concat(frame1)\n",
        "frame2=(df_val_Eng,df_val_span)\n",
        "combined_val=pd.concat(frame2)\n",
        "frame3=(df_test_Eng,df_test_span)\n",
        "combined_test=pd.concat(frame3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg8hl-DO51SP"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "combined_train=shuffle(combined_train)\n",
        "combined_val=shuffle(combined_val)\n",
        "combined_test=shuffle(combined_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MjYrM3nM-BQf",
        "outputId": "5a355f1c-d517-471f-987d-64fbc2cf4fe8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>binary</th>\n",
              "      <th>multiclass</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#USER# #USER# I'm really liking this project, ...</td>\n",
              "      <td>Hope</td>\n",
              "      <td>Realistic Hope</td>\n",
              "      <td>5820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#USER# Oh shit really? I would hope they'd she...</td>\n",
              "      <td>Hope</td>\n",
              "      <td>Generalized Hope</td>\n",
              "      <td>4061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#USER# Good morning, Bud! 🥰 Another good decis...</td>\n",
              "      <td>Hope</td>\n",
              "      <td>Generalized Hope</td>\n",
              "      <td>1621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i aspire to have the level of delusion to beli...</td>\n",
              "      <td>Hope</td>\n",
              "      <td>Unrealistic Hope</td>\n",
              "      <td>1754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#USER# #USER# Projects are continuously attack...</td>\n",
              "      <td>Not Hope</td>\n",
              "      <td>Not Hope</td>\n",
              "      <td>401</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text    binary  \\\n",
              "0  #USER# #USER# I'm really liking this project, ...      Hope   \n",
              "1  #USER# Oh shit really? I would hope they'd she...      Hope   \n",
              "2  #USER# Good morning, Bud! 🥰 Another good decis...      Hope   \n",
              "3  i aspire to have the level of delusion to beli...      Hope   \n",
              "4  #USER# #USER# Projects are continuously attack...  Not Hope   \n",
              "\n",
              "         multiclass    id  \n",
              "0    Realistic Hope  5820  \n",
              "1  Generalized Hope  4061  \n",
              "2  Generalized Hope  1621  \n",
              "3  Unrealistic Hope  1754  \n",
              "4          Not Hope   401  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "w5_tagABR9Ce",
        "outputId": "a1860c1a-bed4-4a1c-9ba5-ee198eee4331"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>binary</th>\n",
              "      <th>multiclass</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mientras me persigno y le rezo a la Virgen del...</td>\n",
              "      <td>Not Hope</td>\n",
              "      <td>Not Hope</td>\n",
              "      <td>5533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>No.. Yo ya no estoy para esperar.. Ni para rog...</td>\n",
              "      <td>Not Hope</td>\n",
              "      <td>Not Hope</td>\n",
              "      <td>549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Creo que estamos ante el mejor episodio de est...</td>\n",
              "      <td>Hope</td>\n",
              "      <td>Generalized Hope</td>\n",
              "      <td>4350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bueno ojalá q cuando llegue este fin de semana...</td>\n",
              "      <td>Hope</td>\n",
              "      <td>Realistic Hope</td>\n",
              "      <td>3593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#USER# #USER# Marcos de boludo no tiene un pel...</td>\n",
              "      <td>Not Hope</td>\n",
              "      <td>Not Hope</td>\n",
              "      <td>770</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text    binary  \\\n",
              "0  Mientras me persigno y le rezo a la Virgen del...  Not Hope   \n",
              "1  No.. Yo ya no estoy para esperar.. Ni para rog...  Not Hope   \n",
              "2  Creo que estamos ante el mejor episodio de est...      Hope   \n",
              "3  Bueno ojalá q cuando llegue este fin de semana...      Hope   \n",
              "4  #USER# #USER# Marcos de boludo no tiene un pel...  Not Hope   \n",
              "\n",
              "         multiclass    id  \n",
              "0          Not Hope  5533  \n",
              "1          Not Hope   549  \n",
              "2  Generalized Hope  4350  \n",
              "3    Realistic Hope  3593  \n",
              "4          Not Hope   770  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_val.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MThQSFPeT3Fb",
        "outputId": "38475627-1f5c-40b1-bcec-a9bc87fa3052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: num2words in c:\\users\\user\\anaconda3\\lib\\site-packages (0.5.13)\n",
            "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from num2words) (0.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install num2words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjug02l2uTMv"
      },
      "outputs": [],
      "source": [
        "#eemot_object = emot.core.emot()\n",
        "ps =PorterStemmer()\n",
        "lemmatiser = WordNetLemmatizer()\n",
        "english_stopwords = stopwords.words('english')\n",
        "exclude = set(string.punctuation)\n",
        "def preprocess(text):\n",
        "  text = contractions.fix(text.lower(), slang=True)\n",
        "  text =re.sub(\"@ ?[A-Za-z0-9_]+\", \" \", text)\n",
        "  text = text.replace('$', ' dollar ')\n",
        "  text=re.sub(r'$', '', text)\n",
        "  text= re.sub(r'’','', text )\n",
        "  text=re.sub('<.*?>','',text)\n",
        "  text=re.sub(r'http\\S+', '', text)\n",
        "  text = ''.join(ch for ch in text if ch not in exclude)\n",
        "  tokens = word_tokenize(text)\n",
        "  #print(\"Tokens:\", tokens)\n",
        "  text = [t for t in tokens if t not in english_stopwords]\n",
        "  text = \" \".join(text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLeISCu-T6b6"
      },
      "outputs": [],
      "source": [
        "import inflect\n",
        "p = inflect.engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9Zk3sqbT8bK"
      },
      "outputs": [],
      "source": [
        "def number_to_words(text):\n",
        "    return ' '.join(p.number_to_words(token) if token.isdigit() else token for token in text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHnvZLcHT9_T"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords, wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZkMXfaVuWYz"
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "#import demoji\n",
        "#demoji.download_codes()\n",
        "def emo(text):\n",
        "  temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
        "  temp=temp.replace(\"_\",\"  \")\n",
        "  return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP8Yx_58uZHu"
      },
      "outputs": [],
      "source": [
        "combined_train['clean_text']=combined_train[\"text\"].apply(lambda x:emo(x))\n",
        "combined_train[\"clean_text\"]=combined_train['clean_text'].apply(lambda X: number_to_words(X))\n",
        "combined_train[\"clean_text\"]=combined_train['clean_text'].apply(lambda X: preprocess(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLSVWE2xub8W"
      },
      "outputs": [],
      "source": [
        "combined_val['clean_text']=combined_val[\"text\"].apply(lambda x:emo(x))\n",
        "combined_val[\"clean_text\"]=combined_val['clean_text'].apply(lambda X: number_to_words(X))\n",
        "combined_val[\"clean_text\"]=combined_val['clean_text'].apply(lambda X: preprocess(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwUDah1U51S5"
      },
      "outputs": [],
      "source": [
        "combined_test['clean_text']=combined_test[\"text\"].apply(lambda x:emo(x))\n",
        "combined_test[\"clean_text\"]=combined_test['clean_text'].apply(lambda X: number_to_words(X))\n",
        "combined_test[\"clean_text\"]=combined_test['clean_text'].apply(lambda X: preprocess(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "TMGOIqdOugPy",
        "outputId": "cdd420d9-03f9-48d4-d529-c263af06ccbb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>binary</th>\n",
              "      <th>multiclass</th>\n",
              "      <th>id</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3656</th>\n",
              "      <td>#USER# #USER# pero si el 5 es hermoso le rezo</td>\n",
              "      <td>Hope</td>\n",
              "      <td>Generalized Hope</td>\n",
              "      <td>2693</td>\n",
              "      <td>user user pero si el five es hermoso le rezo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2051</th>\n",
              "      <td>#USER# Somebody I know who’s been right on eve...</td>\n",
              "      <td>Hope</td>\n",
              "      <td>Realistic Hope</td>\n",
              "      <td>2918</td>\n",
              "      <td>user somebody know right everything else town ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2295</th>\n",
              "      <td>#USER# No me queda duda. Solo pasa hablando de...</td>\n",
              "      <td>Not Hope</td>\n",
              "      <td>Not Hope</td>\n",
              "      <td>361</td>\n",
              "      <td>user queda duda solo pasa hablando de bukele c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1381</th>\n",
              "      <td>#USER# Yeah IG i've found that socializing is ...</td>\n",
              "      <td>Hope</td>\n",
              "      <td>Realistic Hope</td>\n",
              "      <td>8068</td>\n",
              "      <td>user yeah ig found socializing way harder with...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4084</th>\n",
              "      <td>HAD BEEN HOPING SOMEONE RECORDED THIS FROM N1 ...</td>\n",
              "      <td>Not Hope</td>\n",
              "      <td>Not Hope</td>\n",
              "      <td>5683</td>\n",
              "      <td>hoping someone recorded n1 showed fyp url</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text    binary  \\\n",
              "3656      #USER# #USER# pero si el 5 es hermoso le rezo      Hope   \n",
              "2051  #USER# Somebody I know who’s been right on eve...      Hope   \n",
              "2295  #USER# No me queda duda. Solo pasa hablando de...  Not Hope   \n",
              "1381  #USER# Yeah IG i've found that socializing is ...      Hope   \n",
              "4084  HAD BEEN HOPING SOMEONE RECORDED THIS FROM N1 ...  Not Hope   \n",
              "\n",
              "            multiclass    id  \\\n",
              "3656  Generalized Hope  2693   \n",
              "2051    Realistic Hope  2918   \n",
              "2295          Not Hope   361   \n",
              "1381    Realistic Hope  8068   \n",
              "4084          Not Hope  5683   \n",
              "\n",
              "                                             clean_text  \n",
              "3656       user user pero si el five es hermoso le rezo  \n",
              "2051  user somebody know right everything else town ...  \n",
              "2295  user queda duda solo pasa hablando de bukele c...  \n",
              "1381  user yeah ig found socializing way harder with...  \n",
              "4084          hoping someone recorded n1 showed fyp url  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "S6dki-UPupXc",
        "outputId": "d970fb70-7a9c-456f-ea4f-1befa7232394"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "      <th>multiclass</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3656</th>\n",
              "      <td>user user pero si el five es hermoso le rezo</td>\n",
              "      <td>Generalized Hope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2051</th>\n",
              "      <td>user somebody know right everything else town ...</td>\n",
              "      <td>Realistic Hope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2295</th>\n",
              "      <td>user queda duda solo pasa hablando de bukele c...</td>\n",
              "      <td>Not Hope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1381</th>\n",
              "      <td>user yeah ig found socializing way harder with...</td>\n",
              "      <td>Realistic Hope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4084</th>\n",
              "      <td>hoping someone recorded n1 showed fyp url</td>\n",
              "      <td>Not Hope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2422</th>\n",
              "      <td>user user el de la izquierda sonríe así empiez...</td>\n",
              "      <td>Generalized Hope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6360</th>\n",
              "      <td>user pero existe esa mierda es lo peor un anhe...</td>\n",
              "      <td>Not Hope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2927</th>\n",
              "      <td>user user creo que ella es migrante quizás es ...</td>\n",
              "      <td>Not Hope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2870</th>\n",
              "      <td>user user user ojalá nunca ocurriesen casos co...</td>\n",
              "      <td>Unrealistic Hope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5740</th>\n",
              "      <td>el poema gana si adivinamos que es la manifest...</td>\n",
              "      <td>Not Hope</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13095 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             clean_text        multiclass\n",
              "3656       user user pero si el five es hermoso le rezo  Generalized Hope\n",
              "2051  user somebody know right everything else town ...    Realistic Hope\n",
              "2295  user queda duda solo pasa hablando de bukele c...          Not Hope\n",
              "1381  user yeah ig found socializing way harder with...    Realistic Hope\n",
              "4084          hoping someone recorded n1 showed fyp url          Not Hope\n",
              "...                                                 ...               ...\n",
              "2422  user user el de la izquierda sonríe así empiez...  Generalized Hope\n",
              "6360  user pero existe esa mierda es lo peor un anhe...          Not Hope\n",
              "2927  user user creo que ella es migrante quizás es ...          Not Hope\n",
              "2870  user user user ojalá nunca ocurriesen casos co...  Unrealistic Hope\n",
              "5740  el poema gana si adivinamos que es la manifest...          Not Hope\n",
              "\n",
              "[13095 rows x 2 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Move last Column to First Column\n",
        "new_cols = [\"clean_text\",\"multiclass\"]\n",
        "combined_train=combined_train[new_cols]\n",
        "combined_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVzSlyIRRKjd"
      },
      "outputs": [],
      "source": [
        "# # Import label encoder\n",
        "# from sklearn import preprocessing\n",
        "\n",
        "# # label_encoder object knows\n",
        "# # how to understand word labels.\n",
        "# label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# # Encode labels in column 'species'.\n",
        "# combined_train['multiclass']= label_encoder.fit_transform(combined_train['multiclass'])\n",
        "\n",
        "# combined_train['multiclass'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImSiEbkC51TA"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zV9X2Izj51TI",
        "outputId": "6199f577-79a3-4d3b-c437-849bd85c27b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          Text                                       Text_Vectors\n",
            "0  Hello world  [-0.008599066943133836, -0.029534579265209842,...\n",
            "1   Hola mundo  [-0.024901816923374898, -0.03147827928075474, ...\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load embeddings from both Spanish and English files\n",
        "def load_embeddings(file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        next(f)  # Skip the first line (header)\n",
        "        for line in f:\n",
        "            values = line.rstrip().split(' ')\n",
        "            word = values[0]\n",
        "            embedding = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = embedding\n",
        "    return embeddings_index\n",
        "\n",
        "spanish_embeddings_path = 'wiki.multi.es.vec.txt'  # Change this to your Spanish embeddings file path\n",
        "english_embeddings_path = 'wiki.multi.en.vec.txt'  # Change this to your English embeddings file path\n",
        "\n",
        "spanish_embeddings = load_embeddings(spanish_embeddings_path)\n",
        "english_embeddings = load_embeddings(english_embeddings_path)\n",
        "\n",
        "# Step 2: Average embeddings with different shapes\n",
        "def average_embeddings(embeddings_dict):\n",
        "    embedding_size = len(next(iter(embeddings_dict.values())))\n",
        "    mean_embedding = np.zeros(embedding_size)\n",
        "    for embedding in embeddings_dict.values():\n",
        "        mean_embedding += embedding\n",
        "    mean_embedding /= len(embeddings_dict)\n",
        "    return mean_embedding\n",
        "\n",
        "# Calculate average embeddings for both Spanish and English\n",
        "average_spanish_embedding = average_embeddings(spanish_embeddings)\n",
        "average_english_embedding = average_embeddings(english_embeddings)\n",
        "\n",
        "# Step 3: Create a function to generate word vectors from the concatenated embeddings\n",
        "def get_word_vector(word):\n",
        "    if word in spanish_embeddings and word in english_embeddings:\n",
        "        return np.concatenate([spanish_embeddings[word], english_embeddings[word]])\n",
        "    elif word in spanish_embeddings:\n",
        "        return np.concatenate([spanish_embeddings[word], np.zeros_like(average_english_embedding)])\n",
        "    elif word in english_embeddings:\n",
        "        return np.concatenate([np.zeros_like(average_spanish_embedding), english_embeddings[word]])\n",
        "    else:\n",
        "        return np.concatenate([average_spanish_embedding, average_english_embedding])\n",
        "\n",
        "# Step 4: Apply the function to your DataFrame text column\n",
        "def get_text_vectors(text):\n",
        "    words = text.split()\n",
        "    vectors = [get_word_vector(word) for word in words]\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# Example usage with a DataFrame\n",
        "data = {'Text': ['Hello world', 'Hola mundo']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['Text_Vectors'] = df['Text'].apply(get_text_vectors)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqH_Hi2851TK"
      },
      "outputs": [],
      "source": [
        "combined_train['Text_Vectors'] = combined_train['clean_text'].apply(get_text_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVKTqfrJ51TM"
      },
      "outputs": [],
      "source": [
        "combined_val['Text_Vectors'] = combined_val['clean_text'].apply(get_text_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk9fVcwJ51TP"
      },
      "outputs": [],
      "source": [
        "combined_test['Text_Vectors'] = combined_test['clean_text'].apply(get_text_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryhSBAgr51TQ"
      },
      "outputs": [],
      "source": [
        "X_train=combined_train['Text_Vectors']\n",
        "y_train=combined_train['multiclass']\n",
        "X_val=combined_val['Text_Vectors']\n",
        "y_val=combined_val['multiclass']\n",
        "X_test=combined_test['Text_Vectors']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQunQJvZ51TR"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm, tree\n",
        "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from numpy import loadtxt\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMvYTpNb51TU",
        "outputId": "9706287d-5917-475d-a0e1-2a3352820157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Generalized Hope       0.61      0.44      0.51       486\n",
            "        Not Hope       0.69      0.95      0.80      1301\n",
            "  Realistic Hope       0.88      0.03      0.07       202\n",
            "Unrealistic Hope       0.64      0.07      0.13       193\n",
            "\n",
            "        accuracy                           0.68      2182\n",
            "       macro avg       0.70      0.38      0.38      2182\n",
            "    weighted avg       0.68      0.68      0.61      2182\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "lr_model = LogisticRegression(random_state=42)\n",
        "lr_model.fit(X_train.tolist(), y_train.tolist())\n",
        "y_pred_lr = lr_model.predict(X_val.tolist())\n",
        "print( classification_report(y_val, y_pred_lr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoIIUm6x51TW",
        "outputId": "162f7ecc-0741-458a-8dc2-470663b2ad80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Generalized Hope       0.33      0.53      0.41       486\n",
            "        Not Hope       0.70      0.60      0.64      1301\n",
            "  Realistic Hope       0.19      0.24      0.21       202\n",
            "Unrealistic Hope       0.33      0.04      0.07       193\n",
            "\n",
            "        accuracy                           0.50      2182\n",
            "       macro avg       0.39      0.35      0.33      2182\n",
            "    weighted avg       0.53      0.50      0.50      2182\n",
            "\n"
          ]
        }
      ],
      "source": [
        "MNB = GaussianNB()\n",
        "MNB.fit(X_train.tolist(), y_train.tolist())\n",
        "y_pred_MNB= MNB.predict(X_val.tolist())\n",
        "print( classification_report(y_val, y_pred_MNB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6HZdDx951TX",
        "outputId": "ad428946-dffd-4b91-e127-99a5ec8bfb64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Generalized Hope       0.64      0.42      0.50       486\n",
            "        Not Hope       0.68      0.97      0.80      1301\n",
            "  Realistic Hope       0.00      0.00      0.00       202\n",
            "Unrealistic Hope       0.71      0.03      0.05       193\n",
            "\n",
            "        accuracy                           0.67      2182\n",
            "       macro avg       0.51      0.35      0.34      2182\n",
            "    weighted avg       0.61      0.67      0.59      2182\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "svm_model = SVC(kernel='linear', C=1.0)\n",
        "svm_model.fit(X_train.tolist(), y_train.tolist())\n",
        "y_pred_svm = svm_model.predict(X_val.tolist())\n",
        "print( classification_report(y_val, y_pred_svm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii2E27Vh51Tb",
        "outputId": "fd20cbd8-806f-47a5-ed03-48120a737d55"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Generalized Hope       0.59      0.55      0.57       486\n",
            "        Not Hope       0.78      0.84      0.81      1301\n",
            "  Realistic Hope       0.39      0.31      0.35       202\n",
            "Unrealistic Hope       0.42      0.39      0.41       193\n",
            "\n",
            "        accuracy                           0.69      2182\n",
            "       macro avg       0.55      0.52      0.53      2182\n",
            "    weighted avg       0.67      0.69      0.68      2182\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "lsvc_model = LinearSVC(C=1.0, class_weight='balanced')\n",
        "lsvc_model.fit(X_train.tolist(), y_train.tolist())\n",
        "y_pred_lsvc = lsvc_model.predict(X_val.tolist())\n",
        "print( classification_report(y_val, y_pred_lsvc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu6t_vn851Tc"
      },
      "outputs": [],
      "source": [
        "y_pred_lsvc_test = lsvc_model.predict(X_test.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFjleweK51Td"
      },
      "outputs": [],
      "source": [
        "#Submission\n",
        "# to create .csv file consisting of tweet ids and predicted labels with CTbert\n",
        "pred_testmus = pd.DataFrame(data=y_pred_lsvc_test, columns=['Prediction'])\n",
        "submission_t2 = pd.DataFrame()\n",
        "submission_t2['id'] = combined_test['id']\n",
        "submission_t2['multiclass'] = pred_testmus\n",
        "submission_t2.to_csv('predmembeddmultilsvc53.csv', index = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh4w3dET51Te",
        "outputId": "849bfdbb-ffd2-4e9c-b0c4-69fc2ad1c6f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Generalized Hope       0.56      0.29      0.38       486\n",
            "        Not Hope       0.65      0.96      0.77      1301\n",
            "  Realistic Hope       0.00      0.00      0.00       202\n",
            "Unrealistic Hope       0.50      0.01      0.02       193\n",
            "\n",
            "        accuracy                           0.64      2182\n",
            "       macro avg       0.43      0.31      0.29      2182\n",
            "    weighted avg       0.55      0.64      0.55      2182\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "RF_model = RandomForestClassifier()\n",
        "RF_model.fit(X_train.tolist(), y_train.tolist())\n",
        "y_pred_rf = RF_model.predict(X_val.tolist())\n",
        "print( classification_report(y_val, y_pred_rf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrSsc4nT51Tj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import KeyedVectors  # Assuming you have the Muse embeddings in Word2Vec format\n",
        "\n",
        "# Step 1: Load Muse multilingual word embeddings for both Spanish and English\n",
        "# Step 1: Load Muse multilingual word embeddings for both Spanish and English\n",
        "spanish_embeddings_path = 'wiki.es.align.vec'\n",
        "english_embeddings_path = 'wiki.en.align.vec'\n",
        "\n",
        "spanish_embeddings = KeyedVectors.load_word2vec_format(spanish_embeddings_path)\n",
        "english_embeddings = KeyedVectors.load_word2vec_format(english_embeddings_path)\n",
        "\n",
        "# spanish_embeddings_path = 'wiki.es.align.vec'\n",
        "# english_embeddings_path = 'wiki.en.align.vec'\n",
        "\n",
        "# spanish_embeddings = KeyedVectors.load_word2vec_format(spanish_embeddings_path, binary=True)\n",
        "# english_embeddings = KeyedVectors.load_word2vec_format(english_embeddings_path, binary=True)\n",
        "\n",
        "# Step 2: Preprocess the text data and map each word to its corresponding embedding vector\n",
        "def text_to_embeddings(text, embeddings):\n",
        "    words = text.split()\n",
        "    embedding_vectors = []\n",
        "    for word in words:\n",
        "        if word in embeddings:\n",
        "            embedding_vectors.append(embeddings[word])\n",
        "    if embedding_vectors:\n",
        "        return np.mean(embedding_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(embeddings.vector_size)  # Return zeros if no embeddings found for any word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VskXei2X51Tk"
      },
      "outputs": [],
      "source": [
        "# Step 3: Load your dataset containing both Spanish and English texts\n",
        "# Replace 'your_dataset.csv' with the path to your dataset\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Step 4: Apply text_to_embeddings function to each text in the dataset\n",
        "combined_train['Embeddings'] = combined_train['clean_text'].apply(lambda x: text_to_embeddings(x, spanish_embeddings) if 'spanish' in x.lower() else text_to_embeddings(x, english_embeddings))\n",
        "\n",
        "# Step 5: Split the dataset into training and testing sets\n",
        "X = np.stack(combined_train['Embeddings'].to_numpy())\n",
        "y = combined_train['multiclass'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQBi2IRb51Tl"
      },
      "outputs": [],
      "source": [
        "combined_val['Embeddings'] = combined_val['clean_text'].apply(lambda x: text_to_embeddings(x, spanish_embeddings) if 'spanish' in x.lower() else text_to_embeddings(x, english_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HngWaRL51Tp"
      },
      "outputs": [],
      "source": [
        "X_val = np.stack(combined_val['Embeddings'].to_numpy())\n",
        "y_val = combined_val['multiclass'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9wRwYuO51Tq"
      },
      "outputs": [],
      "source": [
        "combined_test['Embeddings'] = combined_test['clean_text'].apply(lambda x: text_to_embeddings(x, spanish_embeddings) if 'spanish' in x.lower() else text_to_embeddings(x, english_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORcDrRZ451Tr"
      },
      "outputs": [],
      "source": [
        "X_train=X\n",
        "X_test=np.stack(combined_test['Embeddings'].to_numpy())\n",
        "y_train=y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDUWchos51Ts",
        "outputId": "f936ac04-e503-482f-ad8f-1f3d53445355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Generalized Hope       0.57      0.40      0.47       486\n",
            "        Not Hope       0.68      0.96      0.79      1301\n",
            "  Realistic Hope       1.00      0.01      0.02       202\n",
            "Unrealistic Hope       0.75      0.05      0.09       193\n",
            "\n",
            "        accuracy                           0.66      2182\n",
            "       macro avg       0.75      0.35      0.34      2182\n",
            "    weighted avg       0.69      0.66      0.59      2182\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Train a Logistic Regression classifier\n",
        "lr_classifier = LogisticRegression(max_iter=1000)\n",
        "lr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Evaluate the classifier on the test set\n",
        "y_pred = lr_classifier.predict(X_val)\n",
        "print(classification_report(y_val, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJwKy4dn51Tv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}