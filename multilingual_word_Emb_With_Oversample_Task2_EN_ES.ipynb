{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FH-DUyD6pU5E","outputId":"cf34e61b-78ad-40e6-f510-ed233485f18e","executionInfo":{"status":"ok","timestamp":1715604167835,"user_tz":-330,"elapsed":125625,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.63.0)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n","Collecting tensorflow-text\n","  Downloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow<2.17,>=2.16.1 (from tensorflow-text)\n","  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.2.0)\n","Collecting h5py>=3.10.0 (from tensorflow<2.17,>=2.16.1->tensorflow-text)\n","  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (18.1.1)\n","Collecting ml-dtypes~=0.3.1 (from tensorflow<2.17,>=2.16.1->tensorflow-text)\n","  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.31.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (4.11.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.14.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.63.0)\n","Collecting tensorboard<2.17,>=2.16 (from tensorflow<2.17,>=2.16.1->tensorflow-text)\n","  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras>=3.0.0 (from tensorflow<2.17,>=2.16.1->tensorflow-text)\n","  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (0.37.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text) (1.25.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.43.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (13.7.1)\n","Collecting namex (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text)\n","  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n","Collecting optree (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text)\n","  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2024.2.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text) (0.1.2)\n","Installing collected packages: namex, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow, tensorflow-text\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.2.0\n","    Uninstalling ml-dtypes-0.2.0:\n","      Successfully uninstalled ml-dtypes-0.2.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.9.0\n","    Uninstalling h5py-3.9.0:\n","      Successfully uninstalled h5py-3.9.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.15.2\n","    Uninstalling tensorboard-2.15.2:\n","      Successfully uninstalled tensorboard-2.15.2\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.15.0\n","    Uninstalling tensorflow-2.15.0:\n","      Successfully uninstalled tensorflow-2.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-text-2.16.1\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n","Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Collecting textsearch>=0.0.21 (from contractions)\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Collecting anyascii (from textsearch>=0.0.21->contractions)\n","  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n","  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n","Collecting emoji\n","  Downloading emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: emoji\n","Successfully installed emoji-2.11.1\n","Collecting emot\n","  Downloading emot-3.1-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: emot\n","Successfully installed emot-3.1\n","Collecting demoji\n","  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m789.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: demoji\n","Successfully installed demoji-1.1.0\n","Collecting nlpaug\n","  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.25.2)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.0.3)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.31.0)\n","Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.1.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.14.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.2.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n","Installing collected packages: nlpaug\n","Successfully installed nlpaug-1.1.11\n"]}],"source":["!pip install tensorflow\n","!pip install tensorflow-text\n","!pip install spacy\n","!pip install contractions\n","!pip install emoji\n","!pip install emot\n","!pip install demoji\n","!pip install nlpaug"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MlrqbU2YrfM6","outputId":"949dc10e-a0c1-43ce-dba2-6ceea1c41006","executionInfo":{"status":"ok","timestamp":1715604226758,"user_tz":-330,"elapsed":58926,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.30.2\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2024.2.2)\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.19.1\n","    Uninstalling tokenizers-0.19.1:\n","      Successfully uninstalled tokenizers-0.19.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.40.2\n","    Uninstalling transformers-4.40.2:\n","      Successfully uninstalled transformers-4.40.2\n","Successfully installed tokenizers-0.13.3 transformers-4.30.2\n","Collecting simpletransformers\n","  Downloading simpletransformers-0.70.0-py3-none-any.whl (315 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.25.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.31.0)\n","Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.66.4)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2023.12.25)\n","Collecting transformers>=4.31.0 (from simpletransformers)\n","  Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets (from simpletransformers)\n","  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.11.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n","Collecting seqeval (from simpletransformers)\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.16.2)\n","Collecting tensorboardx (from simpletransformers)\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.0.3)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.13.3)\n","Collecting wandb>=0.10.32 (from simpletransformers)\n","  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting streamlit (from simpletransformers)\n","  Downloading streamlit-1.34.0-py2.py3-none-any.whl (8.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.1.99)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.20.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (6.0.1)\n","Collecting tokenizers (from simpletransformers)\n","  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.4.3)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n","Collecting docker-pycreds>=0.4.0 (from wandb>=0.10.32->simpletransformers)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.10.32->simpletransformers)\n","  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (4.2.1)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb>=0.10.32->simpletransformers)\n","  Downloading sentry_sdk-2.1.1-py2.py3-none-any.whl (277 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.3/277.3 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setproctitle (from wandb>=0.10.32->simpletransformers)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2024.2.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets->simpletransformers)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash (from datasets->simpletransformers)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets->simpletransformers)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.9.5)\n","Collecting huggingface-hub<1.0,>=0.19.3 (from transformers>=4.31.0->simpletransformers)\n","  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2024.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (3.5.0)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.2.2)\n","Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit->simpletransformers) (1.4)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.3.3)\n","Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (9.4.0)\n","Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (13.7.1)\n","Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (8.3.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.11.0)\n","Collecting pydeck<1,>=0.8.0b4 (from streamlit->simpletransformers)\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.3.3)\n","Collecting watchdog>=2.1.5 (from streamlit->simpletransformers)\n","  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.63.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.6)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.0.3)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.3)\n","Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.16.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.5)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.18.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=b65d02d97a3304bdbce8ed06fe2b8e099b98a26b6d33a07eaefef6cba5ec57a7\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: xxhash, watchdog, tensorboardx, smmap, setproctitle, sentry-sdk, docker-pycreds, dill, pydeck, multiprocess, huggingface-hub, gitdb, tokenizers, seqeval, gitpython, wandb, transformers, datasets, streamlit, simpletransformers\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.20.3\n","    Uninstalling huggingface-hub-0.20.3:\n","      Successfully uninstalled huggingface-hub-0.20.3\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.13.3\n","    Uninstalling tokenizers-0.13.3:\n","      Successfully uninstalled tokenizers-0.13.3\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.30.2\n","    Uninstalling transformers-4.30.2:\n","      Successfully uninstalled transformers-4.30.2\n","Successfully installed datasets-2.19.1 dill-0.3.8 docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 huggingface-hub-0.23.0 multiprocess-0.70.16 pydeck-0.9.1 sentry-sdk-2.1.1 seqeval-1.2.2 setproctitle-1.3.3 simpletransformers-0.70.0 smmap-5.0.1 streamlit-1.34.0 tensorboardx-2.6.2.2 tokenizers-0.19.1 transformers-4.40.2 wandb-0.17.0 watchdog-4.0.0 xxhash-3.4.1\n"]}],"source":["!pip install transformers==4.30.2\n","!pip install simpletransformers"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jxxVhV94peyB","outputId":"b9590e3f-a44e-42b9-e7c9-f6143b8dcfc0","executionInfo":{"status":"ok","timestamp":1715604252427,"user_tz":-330,"elapsed":25671,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-e71742f1a665>:24: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n","  demoji.download_codes()\n"]}],"source":["from simpletransformers.classification import ClassificationModel, ClassificationArgs\n","import pandas as pd\n","\n","import spacy\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","import string\n","import contractions\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","import re\n","from nltk.tokenize import TweetTokenizer\n","import regex\n","import nltk\n","from nltk.stem import SnowballStemmer\n","import demoji\n","import emoji\n","from sklearn.metrics import f1_score\n","import os\n","# os.environ[\"MODEL_DIR\"] = '/content/drive/MyDrive/homo/Model'\n","# import nlpaug.augmenter.char as nac\n","# import nlpaug.augmenter.word as naw\n","from nlpaug.util import Action\n","demoji.download_codes()\n","lemmatizer = WordNetLemmatizer()\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import classification_report,accuracy_score\n","from sklearn.svm import LinearSVC"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_p17NXW_pldt","outputId":"5ed3b913-530f-4681-d799-bcf5effac1ba","executionInfo":{"status":"ok","timestamp":1715604253462,"user_tz":-330,"elapsed":1038,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Error loading whitespace: Package 'whitespace' not found\n","[nltk_data]     in index\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}],"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('whitespace')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","stop_words = set(stopwords.words(\"english\",\"spanish\"))"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ehY5kDex-u34","executionInfo":{"status":"ok","timestamp":1715604345635,"user_tz":-330,"elapsed":85749,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}},"outputId":"83df4706-600e-48c9-fae4-176b78db8f11"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"0cil9ooIpqgX","executionInfo":{"status":"ok","timestamp":1715604422099,"user_tz":-330,"elapsed":1709,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}}},"outputs":[],"source":["df_train_Eng = pd.read_csv('/content/drive/MyDrive/HOPE2024/Hope_Task2_Eng/Task 2 Training set (English).csv')\n","df_val_Eng=pd.read_csv('/content/drive/MyDrive/HOPE2024/Hope_Task2_Eng/val.csv')\n","df_train_span=pd.read_csv('/content/drive/MyDrive/HOPE2024/Hope_Task2_Span/train.csv')\n","df_val_span=pd.read_csv('/content/drive/MyDrive/HOPE2024/Hope_Task2_Span/val.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JwIxZ6V1Ec_8"},"outputs":[],"source":["df_test_Eng=pd.read_csv('test - no_label.csv')\n","df_test_span=pd.read_csv('Task2_ES_test_no_label.csv')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Tzb2pnXAEc_8","executionInfo":{"status":"ok","timestamp":1715604567158,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}}},"outputs":[],"source":["frame1=(df_train_Eng,df_train_span)\n","combined_train=pd.concat(frame1)\n","frame2=(df_val_Eng,df_val_span)\n","combined_val=pd.concat(frame2)\n","# frame3=(df_test_Eng,df_test_span)\n","# combined_test=pd.concat(frame3)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"C3k9rZp0Ec_8","executionInfo":{"status":"ok","timestamp":1715604577402,"user_tz":-330,"elapsed":369,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}}},"outputs":[],"source":["from sklearn.utils import shuffle\n","combined_train=shuffle(combined_train)\n","combined_val=shuffle(combined_val)\n","# combined_test=shuffle(combined_test)"]},{"cell_type":"code","source":["combined_train.multiclass.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0IKflm4AG_U","executionInfo":{"status":"ok","timestamp":1715604594286,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}},"outputId":"f0f8b635-4466-450f-d674-7bb9ec3e7aa6"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["multiclass\n","Not Hope            7789\n","Generalized Hope    2877\n","Realistic Hope      1235\n","Unrealistic Hope    1194\n","Name: count, dtype: int64"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["len(combined_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mr2tJH8xDryn","executionInfo":{"status":"ok","timestamp":1715605540761,"user_tz":-330,"elapsed":360,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}},"outputId":"01ba3af7-9e90-452f-a2b2-6f34907835f9"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13095"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"MjYrM3nM-BQf","outputId":"23707be0-28e4-49dc-a08b-df5641c53730","executionInfo":{"status":"ok","timestamp":1715604570839,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   text    binary  \\\n","3989  #USER# A Francisco le pido y en Gabriela lo ll...  Not Hope   \n","4322  I fucking aspire to be this rich where I can j...      Hope   \n","5482  FRASE DEL DÍA:  “Brayan Bello tiene gran actit...  Not Hope   \n","4909  Si pudiera pedir un deseo desearía que fueras ...      Hope   \n","3882  Más de una semana rogando para que #USER# de r...  Not Hope   \n","\n","            multiclass    id  \n","3989          Not Hope  3671  \n","4322  Unrealistic Hope  3093  \n","5482          Not Hope  4612  \n","4909  Unrealistic Hope  3673  \n","3882          Not Hope  8037  "],"text/html":["\n","  <div id=\"df-eabf1841-12e6-42dd-a226-29982ae5575d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>binary</th>\n","      <th>multiclass</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3989</th>\n","      <td>#USER# A Francisco le pido y en Gabriela lo ll...</td>\n","      <td>Not Hope</td>\n","      <td>Not Hope</td>\n","      <td>3671</td>\n","    </tr>\n","    <tr>\n","      <th>4322</th>\n","      <td>I fucking aspire to be this rich where I can j...</td>\n","      <td>Hope</td>\n","      <td>Unrealistic Hope</td>\n","      <td>3093</td>\n","    </tr>\n","    <tr>\n","      <th>5482</th>\n","      <td>FRASE DEL DÍA:  “Brayan Bello tiene gran actit...</td>\n","      <td>Not Hope</td>\n","      <td>Not Hope</td>\n","      <td>4612</td>\n","    </tr>\n","    <tr>\n","      <th>4909</th>\n","      <td>Si pudiera pedir un deseo desearía que fueras ...</td>\n","      <td>Hope</td>\n","      <td>Unrealistic Hope</td>\n","      <td>3673</td>\n","    </tr>\n","    <tr>\n","      <th>3882</th>\n","      <td>Más de una semana rogando para que #USER# de r...</td>\n","      <td>Not Hope</td>\n","      <td>Not Hope</td>\n","      <td>8037</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eabf1841-12e6-42dd-a226-29982ae5575d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-eabf1841-12e6-42dd-a226-29982ae5575d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-eabf1841-12e6-42dd-a226-29982ae5575d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-84c54cd6-558b-4f7c-8a83-42134208b10a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-84c54cd6-558b-4f7c-8a83-42134208b10a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-84c54cd6-558b-4f7c-8a83-42134208b10a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"combined_train","summary":"{\n  \"name\": \"combined_train\",\n  \"rows\": 13095,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13095,\n        \"samples\": [\n          \"#USER# run is better tho i wish he did run more (this is not because i need content for yangling daily)\",\n          \"#USER# #USER# Why on earth would anyone expect our leaders to do what needs to be done?\",\n          \"Today\\u2019s after walk treat is from our beloved friend and neighbor , #Tazwellbrown #Tazwellbrowne. Following his example, we aspire to be kind and more loving, even when the small dogs of the world bark at us. #URL#\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"binary\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Hope\",\n          \"Not Hope\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"multiclass\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Unrealistic Hope\",\n          \"Realistic Hope\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2551,\n        \"min\": 0,\n        \"max\": 9204,\n        \"num_unique_values\": 8483,\n        \"samples\": [\n          8959,\n          6106\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":11}],"source":["combined_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"w5_tagABR9Ce","outputId":"a1860c1a-bed4-4a1c-9ba5-ee198eee4331"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>binary</th>\n","      <th>multiclass</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>411</th>\n","      <td>#USER# Tú haces que los cosplays tengan un alg...</td>\n","      <td>Hope</td>\n","      <td>Unrealistic Hope</td>\n","      <td>5293</td>\n","    </tr>\n","    <tr>\n","      <th>1003</th>\n","      <td>I don’t ask for much but I pray I meet someone...</td>\n","      <td>Hope</td>\n","      <td>Realistic Hope</td>\n","      <td>3578</td>\n","    </tr>\n","    <tr>\n","      <th>752</th>\n","      <td>#USER# Cada quien tiene su santo a quien rezar...</td>\n","      <td>Not Hope</td>\n","      <td>Not Hope</td>\n","      <td>233</td>\n","    </tr>\n","    <tr>\n","      <th>304</th>\n","      <td>#USER# #USER# #USER# I mean after seeing Maver...</td>\n","      <td>Not Hope</td>\n","      <td>Not Hope</td>\n","      <td>7621</td>\n","    </tr>\n","    <tr>\n","      <th>892</th>\n","      <td>quien es esa de la izquierda yo solo le rezo a...</td>\n","      <td>Not Hope</td>\n","      <td>Not Hope</td>\n","      <td>3091</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   text    binary  \\\n","411   #USER# Tú haces que los cosplays tengan un alg...      Hope   \n","1003  I don’t ask for much but I pray I meet someone...      Hope   \n","752   #USER# Cada quien tiene su santo a quien rezar...  Not Hope   \n","304   #USER# #USER# #USER# I mean after seeing Maver...  Not Hope   \n","892   quien es esa de la izquierda yo solo le rezo a...  Not Hope   \n","\n","            multiclass    id  \n","411   Unrealistic Hope  5293  \n","1003    Realistic Hope  3578  \n","752           Not Hope   233  \n","304           Not Hope  7621  \n","892           Not Hope  3091  "]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["combined_val.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MThQSFPeT3Fb","outputId":"38475627-1f5c-40b1-bcec-a9bc87fa3052"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: num2words in c:\\users\\user\\anaconda3\\lib\\site-packages (0.5.13)\n","Requirement already satisfied: docopt>=0.6.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from num2words) (0.6.2)\n"]}],"source":["!pip install num2words"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"StPQdmqMEc_8","executionInfo":{"status":"ok","timestamp":1715605269620,"user_tz":-330,"elapsed":399,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}}},"outputs":[],"source":["from sklearn.utils import resample\n","#create two different dataframe of majority and minority class\n","df_majority = combined_train[(combined_train['multiclass']=='Not Hope')]\n","df_minority1 = combined_train[(combined_train['multiclass']=='Generalized Hope')]\n","df_minority2 = combined_train[(combined_train['multiclass']=='Unrealistic Hope')]\n","df_minority3 = combined_train[(combined_train['multiclass']=='Realistic Hope')]\n","\n","# upsample minority class\n","df_minority_upsampled1 = resample(df_minority1,replace=True, n_samples= 7789, random_state=None)  # reproducible results\n","df_minority_upsampled2 = resample(df_minority2,replace=True, n_samples= 7789, random_state=None)\n","df_minority_upsampled3 = resample(df_minority3,replace=True, n_samples= 7789, random_state=None)\n","\n","# Combine majority class with upsampled minority class\n","# Combine majority class with upsampled minority class\n","df_upsampled = pd.concat([df_minority_upsampled1,df_minority_upsampled2,df_minority_upsampled3, df_majority])"]},{"cell_type":"code","source":["len(df_upsampled)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HAO5AqbWDMww","executionInfo":{"status":"ok","timestamp":1715605570183,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}},"outputId":"b946370c-5491-4869-b209-1d74b5b340ac"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["31156"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["df_upsampled.multiclass.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XdSPR812CvFU","executionInfo":{"status":"ok","timestamp":1715605297789,"user_tz":-330,"elapsed":372,"user":{"displayName":"Sonith D","userId":"06125108616349518866"}},"outputId":"fbea4d94-ba09-4c35-98e9-fb81c08615be"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["multiclass\n","Generalized Hope    7789\n","Unrealistic Hope    7789\n","Realistic Hope      7789\n","Not Hope            7789\n","Name: count, dtype: int64"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mjug02l2uTMv"},"outputs":[],"source":["#eemot_object = emot.core.emot()\n","ps =PorterStemmer()\n","lemmatiser = WordNetLemmatizer()\n","english_stopwords = stopwords.words('english')\n","exclude = set(string.punctuation)\n","def preprocess(text):\n","  text = contractions.fix(text.lower(), slang=True)\n","  text =re.sub(\"@ ?[A-Za-z0-9_]+\", \" \", text)\n","  text = text.replace('$', ' dollar ')\n","  text=re.sub(r'$', '', text)\n","  text= re.sub(r'’','', text )\n","  text=re.sub('<.*?>','',text)\n","  text=re.sub(r'http\\S+', '', text)\n","  text = ''.join(ch for ch in text if ch not in exclude)\n","  tokens = word_tokenize(text)\n","  #print(\"Tokens:\", tokens)\n","  text = [t for t in tokens if t not in english_stopwords]\n","  text = \" \".join(text)\n","  return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLeISCu-T6b6"},"outputs":[],"source":["import inflect\n","p = inflect.engine()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9Zk3sqbT8bK"},"outputs":[],"source":["def number_to_words(text):\n","    return ' '.join(p.number_to_words(token) if token.isdigit() else token for token in text.split())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHnvZLcHT9_T"},"outputs":[],"source":["from nltk.corpus import stopwords, wordnet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZkMXfaVuWYz"},"outputs":[],"source":["import emoji\n","#import demoji\n","#demoji.download_codes()\n","def emo(text):\n","  temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n","  temp=temp.replace(\"_\",\"  \")\n","  return temp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VP8Yx_58uZHu"},"outputs":[],"source":["df_upsampled['clean_text']=df_upsampled[\"text\"].apply(lambda x:emo(x))\n","df_upsampled[\"clean_text\"]=df_upsampled['clean_text'].apply(lambda X: number_to_words(X))\n","df_upsampled[\"clean_text\"]=df_upsampled['clean_text'].apply(lambda X: preprocess(X))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLSVWE2xub8W"},"outputs":[],"source":["combined_val['clean_text']=combined_val[\"text\"].apply(lambda x:emo(x))\n","combined_val[\"clean_text\"]=combined_val['clean_text'].apply(lambda X: number_to_words(X))\n","combined_val[\"clean_text\"]=combined_val['clean_text'].apply(lambda X: preprocess(X))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAW6h9ADEc_9"},"outputs":[],"source":["combined_test['clean_text']=combined_test[\"text\"].apply(lambda x:emo(x))\n","combined_test[\"clean_text\"]=combined_test['clean_text'].apply(lambda X: number_to_words(X))\n","combined_test[\"clean_text\"]=combined_test['clean_text'].apply(lambda X: preprocess(X))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCmWYVvEEc_9"},"outputs":[],"source":["df_test_Eng['clean_text']=df_test_Eng[\"text\"].apply(lambda x:emo(x))\n","df_test_Eng[\"clean_text\"]=df_test_Eng['clean_text'].apply(lambda X: number_to_words(X))\n","df_test_Eng[\"clean_text\"]=df_test_Eng['clean_text'].apply(lambda X: preprocess(X))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wc3Z02b4Ec_9"},"outputs":[],"source":["df_test_span['clean_text']=df_test_span[\"text\"].apply(lambda x:emo(x))\n","df_test_span[\"clean_text\"]=df_test_span['clean_text'].apply(lambda X: number_to_words(X))\n","df_test_span[\"clean_text\"]=df_test_span['clean_text'].apply(lambda X: preprocess(X))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"TMGOIqdOugPy","outputId":"cdd420d9-03f9-48d4-d529-c263af06ccbb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>binary</th>\n","      <th>multiclass</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5423</th>\n","      <td>#USER# #USER# Que se puede esperar de un tipo ...</td>\n","      <td>Not Hope</td>\n","      <td>Not Hope</td>\n","      <td>6484</td>\n","    </tr>\n","    <tr>\n","      <th>6491</th>\n","      <td>Hoy es día de patinar con mi novio en la Rambl...</td>\n","      <td>Not Hope</td>\n","      <td>Not Hope</td>\n","      <td>4026</td>\n","    </tr>\n","    <tr>\n","      <th>706</th>\n","      <td>Me acuesto pensando en algo, sueño con eso y m...</td>\n","      <td>Not Hope</td>\n","      <td>Not Hope</td>\n","      <td>4715</td>\n","    </tr>\n","    <tr>\n","      <th>6582</th>\n","      <td>Deseo tanto estar tumbado en la arena de la pl...</td>\n","      <td>Hope</td>\n","      <td>Unrealistic Hope</td>\n","      <td>4216</td>\n","    </tr>\n","    <tr>\n","      <th>2515</th>\n","      <td>Elated to see the people of Kyabigulu in Kinon...</td>\n","      <td>Hope</td>\n","      <td>Realistic Hope</td>\n","      <td>477</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   text    binary  \\\n","5423  #USER# #USER# Que se puede esperar de un tipo ...  Not Hope   \n","6491  Hoy es día de patinar con mi novio en la Rambl...  Not Hope   \n","706   Me acuesto pensando en algo, sueño con eso y m...  Not Hope   \n","6582  Deseo tanto estar tumbado en la arena de la pl...      Hope   \n","2515  Elated to see the people of Kyabigulu in Kinon...      Hope   \n","\n","            multiclass    id  \n","5423          Not Hope  6484  \n","6491          Not Hope  4026  \n","706           Not Hope  4715  \n","6582  Unrealistic Hope  4216  \n","2515    Realistic Hope   477  "]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["combined_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"S6dki-UPupXc","outputId":"d970fb70-7a9c-456f-ea4f-1befa7232394"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clean_text</th>\n","      <th>multiclass</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2397</th>\n","      <td>user sorry fucked hope soon need anything lmk lt3</td>\n","      <td>Generalized Hope</td>\n","    </tr>\n","    <tr>\n","      <th>2189</th>\n","      <td>user porque como tengamos que esperar que tú h...</td>\n","      <td>Generalized Hope</td>\n","    </tr>\n","    <tr>\n","      <th>6195</th>\n","      <td>user le deseo lo mejor yo estaré mejor también...</td>\n","      <td>Generalized Hope</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>precisely love shows yearn place give place fr...</td>\n","      <td>Generalized Hope</td>\n","    </tr>\n","    <tr>\n","      <th>1074</th>\n","      <td>packed ready minnie mouse ears user predicted ...</td>\n","      <td>Generalized Hope</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>889</th>\n","      <td>user la verdad es que le desearía la nadie por...</td>\n","      <td>Not Hope</td>\n","    </tr>\n","    <tr>\n","      <th>5821</th>\n","      <td>creo que necesito ir un curso de como hacer so...</td>\n","      <td>Not Hope</td>\n","    </tr>\n","    <tr>\n","      <th>4154</th>\n","      <td>quiero ir la perra escuela tengo un vergal de ...</td>\n","      <td>Not Hope</td>\n","    </tr>\n","    <tr>\n","      <th>5066</th>\n","      <td>user toda la razón tengo hija pero desearía qu...</td>\n","      <td>Not Hope</td>\n","    </tr>\n","    <tr>\n","      <th>2304</th>\n","      <td>rt user falta mucho para la siesta desmayo del...</td>\n","      <td>Not Hope</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31156 rows × 2 columns</p>\n","</div>"],"text/plain":["                                             clean_text        multiclass\n","2397  user sorry fucked hope soon need anything lmk lt3  Generalized Hope\n","2189  user porque como tengamos que esperar que tú h...  Generalized Hope\n","6195  user le deseo lo mejor yo estaré mejor también...  Generalized Hope\n","27    precisely love shows yearn place give place fr...  Generalized Hope\n","1074  packed ready minnie mouse ears user predicted ...  Generalized Hope\n","...                                                 ...               ...\n","889   user la verdad es que le desearía la nadie por...          Not Hope\n","5821  creo que necesito ir un curso de como hacer so...          Not Hope\n","4154  quiero ir la perra escuela tengo un vergal de ...          Not Hope\n","5066  user toda la razón tengo hija pero desearía qu...          Not Hope\n","2304  rt user falta mucho para la siesta desmayo del...          Not Hope\n","\n","[31156 rows x 2 columns]"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["# Move last Column to First Column\n","new_cols = [\"clean_text\",\"multiclass\"]\n","df_upsampled=df_upsampled[new_cols]\n","df_upsampled"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVzSlyIRRKjd"},"outputs":[],"source":["# # Import label encoder\n","# from sklearn import preprocessing\n","\n","# # label_encoder object knows\n","# # how to understand word labels.\n","# label_encoder = preprocessing.LabelEncoder()\n","\n","# # Encode labels in column 'species'.\n","# combined_train['multiclass']= label_encoder.fit_transform(combined_train['multiclass'])\n","\n","# combined_train['multiclass'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjjRE44LEc_-"},"outputs":[],"source":["import io\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Hcq6t7gEc_-","outputId":"cb5f4774-fcae-4359-e934-bdc4d11a4f83"},"outputs":[{"name":"stdout","output_type":"stream","text":["          Text                                       Text_Vectors\n","0  Hello world  [-0.008599066943133836, -0.029534579265209842,...\n","1   Hola mundo  [-0.024901816923374898, -0.03147827928075474, ...\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","# Step 1: Load embeddings from both Spanish and English files\n","def load_embeddings(file_path):\n","    embeddings_index = {}\n","    with open(file_path, encoding=\"utf-8\") as f:\n","        next(f)  # Skip the first line (header)\n","        for line in f:\n","            values = line.rstrip().split(' ')\n","            word = values[0]\n","            embedding = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = embedding\n","    return embeddings_index\n","\n","spanish_embeddings_path = 'wiki.multi.es.vec.txt'  # Change this to your Spanish embeddings file path\n","english_embeddings_path = 'wiki.multi.en.vec.txt'  # Change this to your English embeddings file path\n","\n","spanish_embeddings = load_embeddings(spanish_embeddings_path)\n","english_embeddings = load_embeddings(english_embeddings_path)\n","\n","# Step 2: Average embeddings with different shapes\n","def average_embeddings(embeddings_dict):\n","    embedding_size = len(next(iter(embeddings_dict.values())))\n","    mean_embedding = np.zeros(embedding_size)\n","    for embedding in embeddings_dict.values():\n","        mean_embedding += embedding\n","    mean_embedding /= len(embeddings_dict)\n","    return mean_embedding\n","\n","# Calculate average embeddings for both Spanish and English\n","average_spanish_embedding = average_embeddings(spanish_embeddings)\n","average_english_embedding = average_embeddings(english_embeddings)\n","\n","# Step 3: Create a function to generate word vectors from the concatenated embeddings\n","def get_word_vector(word):\n","    if word in spanish_embeddings and word in english_embeddings:\n","        return np.concatenate([spanish_embeddings[word], english_embeddings[word]])\n","    elif word in spanish_embeddings:\n","        return np.concatenate([spanish_embeddings[word], np.zeros_like(average_english_embedding)])\n","    elif word in english_embeddings:\n","        return np.concatenate([np.zeros_like(average_spanish_embedding), english_embeddings[word]])\n","    else:\n","        return np.concatenate([average_spanish_embedding, average_english_embedding])\n","\n","# Step 4: Apply the function to your DataFrame text column\n","def get_text_vectors(text):\n","    words = text.split()\n","    vectors = [get_word_vector(word) for word in words]\n","    return np.mean(vectors, axis=0)\n","\n","# Example usage with a DataFrame\n","data = {'Text': ['Hello world', 'Hola mundo']}\n","df = pd.DataFrame(data)\n","\n","df['Text_Vectors'] = df['Text'].apply(get_text_vectors)\n","print(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IuBYb_9cEc_-"},"outputs":[],"source":["df_upsampled['Text_Vectors'] = df_upsampled['clean_text'].apply(get_text_vectors)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFVygfQsEc_-"},"outputs":[],"source":["combined_val['Text_Vectors'] = combined_val['clean_text'].apply(get_text_vectors)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwYojFptEc_-"},"outputs":[],"source":["combined_test['Text_Vectors'] = combined_test['clean_text'].apply(get_text_vectors)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1Zs_DjOEc_-"},"outputs":[],"source":["df_test_span['Text_Vectors'] = df_test_span['clean_text'].apply(get_text_vectors)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yAfEYH4Ec__"},"outputs":[],"source":["df_test_Eng['Text_Vectors'] = df_test_Eng['clean_text'].apply(get_text_vectors)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcW-y3LgEc__"},"outputs":[],"source":["X_train=df_upsampled['Text_Vectors']\n","y_train=df_upsampled['multiclass']\n","X_val=combined_val['Text_Vectors']\n","y_val=combined_val['multiclass']\n","X_test1=df_test_span['Text_Vectors']\n","X_test2=df_test_Eng['Text_Vectors']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHkHqJOVEc__"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn import svm, tree\n","from sklearn.naive_bayes import MultinomialNB,GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.linear_model import Perceptron\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from numpy import loadtxt\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rS_y4stEc__","outputId":"b260313f-8ebd-4787-90c3-e470cbf6c47a"},"outputs":[{"name":"stdout","output_type":"stream","text":["                  precision    recall  f1-score   support\n","\n","Generalized Hope       0.57      0.56      0.56       486\n","        Not Hope       0.83      0.66      0.74      1301\n","  Realistic Hope       0.30      0.52      0.38       202\n","Unrealistic Hope       0.28      0.49      0.36       193\n","\n","        accuracy                           0.61      2182\n","       macro avg       0.50      0.56      0.51      2182\n","    weighted avg       0.68      0.61      0.63      2182\n","\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}],"source":["lr_model = LogisticRegression(random_state=42)\n","lr_model.fit(X_train.tolist(), y_train.tolist())\n","y_pred_lr = lr_model.predict(X_val.tolist())\n","print( classification_report(y_val, y_pred_lr))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWNn-SfmEc__"},"outputs":[],"source":["y_pred_lr_test1 = lr_model.predict(X_test1.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJZBDTYrEc__"},"outputs":[],"source":["y_pred_lr_test2 = lr_model.predict(X_test2.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZEOgjVDEc__"},"outputs":[],"source":["#Submission\n","# to create .csv file consisting of tweet ids and predicted labels with CTbert\n","pred_testmus = pd.DataFrame(data=y_pred_lr_test1, columns=['Prediction'])\n","submission_t2 = pd.DataFrame()\n","submission_t2['id'] = df_test_span['id']\n","submission_t2['multiclass'] = pred_testmus\n","submission_t2.to_csv('pred_multiemb_span_multiclass_lr_16th.csv', index = None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XalnVE-fEc__"},"outputs":[],"source":["#Submission\n","# to create .csv file consisting of tweet ids and predicted labels with CTbert\n","pred_testmus = pd.DataFrame(data=y_pred_lr_test2, columns=['Prediction'])\n","submission_t2 = pd.DataFrame()\n","submission_t2['id'] = df_test_Eng['id']\n","submission_t2['multiclass'] = pred_testmus\n","submission_t2.to_csv('pred_multiemb_eng_multiclass_lr_16th.csv', index = None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVdPUAJVEdAE","outputId":"f255afa4-b0c8-49c2-dcb7-4b591727b2fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["                  precision    recall  f1-score   support\n","\n","Generalized Hope       0.35      0.45      0.39       486\n","        Not Hope       0.69      0.59      0.64      1301\n","  Realistic Hope       0.18      0.34      0.23       202\n","Unrealistic Hope       0.30      0.07      0.12       193\n","\n","        accuracy                           0.49      2182\n","       macro avg       0.38      0.36      0.35      2182\n","    weighted avg       0.53      0.49      0.50      2182\n","\n"]}],"source":["MNB = GaussianNB()\n","MNB.fit(X_train.tolist(), y_train.tolist())\n","y_pred_MNB= MNB.predict(X_val.tolist())\n","print( classification_report(y_val, y_pred_MNB))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YO683fq_EdAE","outputId":"00c97f89-4f49-4c91-d710-c757118bd7fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["                  precision    recall  f1-score   support\n","\n","Generalized Hope       0.57      0.55      0.56       486\n","        Not Hope       0.83      0.65      0.73      1301\n","  Realistic Hope       0.31      0.52      0.39       202\n","Unrealistic Hope       0.28      0.50      0.36       193\n","\n","        accuracy                           0.60      2182\n","       macro avg       0.50      0.56      0.51      2182\n","    weighted avg       0.68      0.60      0.63      2182\n","\n"]}],"source":["svm_model = SVC(kernel='linear', C=1.0)\n","svm_model.fit(X_train.tolist(), y_train.tolist())\n","y_pred_svm = svm_model.predict(X_val.tolist())\n","print( classification_report(y_val, y_pred_svm))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unhYL0y_EdAE","outputId":"02eb2394-212d-4ddc-868c-a78aa0f6ae75"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["                  precision    recall  f1-score   support\n","\n","Generalized Hope       0.55      0.52      0.54       486\n","        Not Hope       0.82      0.67      0.74      1301\n","  Realistic Hope       0.30      0.48      0.37       202\n","Unrealistic Hope       0.29      0.50      0.37       193\n","\n","        accuracy                           0.61      2182\n","       macro avg       0.49      0.54      0.50      2182\n","    weighted avg       0.67      0.61      0.63      2182\n","\n"]}],"source":["from sklearn.svm import LinearSVC\n","lsvc_model = LinearSVC(C=1.0, class_weight='balanced')\n","lsvc_model.fit(X_train.tolist(), y_train.tolist())\n","y_pred_lsvc = lsvc_model.predict(X_val.tolist())\n","print( classification_report(y_val, y_pred_lsvc))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYBtcq0ZEdAE"},"outputs":[],"source":["y_pred_lsvc_test1 = lsvc_model.predict(X_test1.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J43U46ZiEdAE"},"outputs":[],"source":["y_pred_lsvc_test2 = lsvc_model.predict(X_test2.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ozn4Z7udEdAE"},"outputs":[],"source":["#Submission\n","# to create .csv file consisting of tweet ids and predicted labels with CTbert\n","pred_testmus = pd.DataFrame(data=y_pred_lsvc_test1, columns=['Prediction'])\n","submission_t2 = pd.DataFrame()\n","submission_t2['id'] = df_test_span['id']\n","submission_t2['multiclass'] = pred_testmus\n","submission_t2.to_csv('predmembeddmultilsvc_span16th.csv', index = None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URfOEtJEEdAE"},"outputs":[],"source":["#Submission\n","# to create .csv file consisting of tweet ids and predicted labels with CTbert\n","pred_testmus = pd.DataFrame(data=y_pred_lsvc_test2, columns=['Prediction'])\n","submission_t2 = pd.DataFrame()\n","submission_t2['id'] = df_test_Eng['id']\n","submission_t2['multiclass'] = pred_testmus\n","submission_t2.to_csv('predmembeddmultilsvcEng16th.csv', index = None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oGCRWZ2wEdAF","outputId":"a16b6226-5f3f-4f9e-e636-63c70a3f149b"},"outputs":[{"name":"stdout","output_type":"stream","text":["                  precision    recall  f1-score   support\n","\n","Generalized Hope       0.58      0.32      0.42       486\n","        Not Hope       0.66      0.96      0.78      1301\n","  Realistic Hope       0.00      0.00      0.00       202\n","Unrealistic Hope       0.67      0.06      0.11       193\n","\n","        accuracy                           0.65      2182\n","       macro avg       0.48      0.34      0.33      2182\n","    weighted avg       0.58      0.65      0.57      2182\n","\n"]}],"source":["RF_model = RandomForestClassifier()\n","RF_model.fit(X_train.tolist(), y_train.tolist())\n","y_pred_rf = RF_model.predict(X_val.tolist())\n","print( classification_report(y_val, y_pred_rf))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIxZzxo0EdAF"},"outputs":[],"source":["#Code alignedVector\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","from gensim.models import KeyedVectors  # Assuming you have the Muse embeddings in Word2Vec format\n","\n","# Step 1: Load Muse multilingual word embeddings for both Spanish and English\n","# Step 1: Load Muse multilingual word embeddings for both Spanish and English\n","spanish_embeddings_path = 'wiki.es.align.vec'\n","english_embeddings_path = 'wiki.en.align.vec'\n","\n","spanish_embeddings = KeyedVectors.load_word2vec_format(spanish_embeddings_path)\n","english_embeddings = KeyedVectors.load_word2vec_format(english_embeddings_path)\n","\n","# spanish_embeddings_path = 'wiki.es.align.vec'\n","# english_embeddings_path = 'wiki.en.align.vec'\n","\n","# spanish_embeddings = KeyedVectors.load_word2vec_format(spanish_embeddings_path, binary=True)\n","# english_embeddings = KeyedVectors.load_word2vec_format(english_embeddings_path, binary=True)\n","\n","# Step 2: Preprocess the text data and map each word to its corresponding embedding vector\n","def text_to_embeddings(text, embeddings):\n","    words = text.split()\n","    embedding_vectors = []\n","    for word in words:\n","        if word in embeddings:\n","            embedding_vectors.append(embeddings[word])\n","    if embedding_vectors:\n","        return np.mean(embedding_vectors, axis=0)\n","    else:\n","        return np.zeros(embeddings.vector_size)  # Return zeros if no embeddings found for any word"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hofKkta7EdAF"},"outputs":[],"source":["# Step 3: Load your dataset containing both Spanish and English texts\n","# Replace 'your_dataset.csv' with the path to your dataset\n","# df = pd.read_csv('your_dataset.csv')\n","\n","# Step 4: Apply text_to_embeddings function to each text in the dataset\n","df_upsampled['Embeddings'] = df_upsampled['clean_text'].apply(lambda x: text_to_embeddings(x, spanish_embeddings) if 'spanish' in x.lower() else text_to_embeddings(x, english_embeddings))\n","\n","# Step 5: Split the dataset into training and testing sets\n","X = np.stack(df_upsampled['Embeddings'].to_numpy())\n","y = df_upsampled['multiclass'].to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yM5N848BEdAF"},"outputs":[],"source":["combined_val['Embeddings'] = combined_val['clean_text'].apply(lambda x: text_to_embeddings(x, spanish_embeddings) if 'spanish' in x.lower() else text_to_embeddings(x, english_embeddings))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crhRYSE2EdAF"},"outputs":[],"source":["X_val = np.stack(combined_val['Embeddings'].to_numpy())\n","y_val = combined_val['multiclass'].to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhPUQLthEdAF"},"outputs":[],"source":["combined_test['Embeddings'] = combined_test['clean_text'].apply(lambda x: text_to_embeddings(x, spanish_embeddings) if 'spanish' in x.lower() else text_to_embeddings(x, english_embeddings))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LVXqRN_cEdAF"},"outputs":[],"source":["df_test_span['Embeddings'] = df_test_span['clean_text'].apply(lambda x: text_to_embeddings(x, spanish_embeddings) if 'spanish' in x.lower() else text_to_embeddings(x, english_embeddings))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gA8oowheEdAF"},"outputs":[],"source":["df_test_Eng['Embeddings'] = df_test_Eng['clean_text'].apply(lambda x: text_to_embeddings(x, spanish_embeddings) if 'spanish' in x.lower() else text_to_embeddings(x, english_embeddings))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wb78A3bwEdAF"},"outputs":[],"source":["X_train=X\n","X_test1=np.stack(df_test_span['Embeddings'].to_numpy())\n","X_test2=np.stack(df_test_Eng['Embeddings'].to_numpy())\n","y_train=y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeEuZM-uEdAF","outputId":"59455452-ceff-4c3c-d1c7-15039e733a68"},"outputs":[{"ename":"ValueError","evalue":"setting an array element with a sequence.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[114], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m lr_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Step 7: Evaluate the classifier on the test set\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m lr_classifier\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_val, y_pred))\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:451\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    450\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 451\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    453\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:432\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    429\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    430\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 432\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    433\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:893\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m    847\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;124;03m    Return the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;124;03m          dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 893\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values, dtype)\n","\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."]}],"source":["# Step 6: Train a Logistic Regression classifier\n","lr_classifier = LogisticRegression(max_iter=1000)\n","lr_classifier.fit(X_train, y_train)\n","\n","# Step 7: Evaluate the classifier on the test set\n","y_pred = lr_classifier.predict(X_val)\n","print(classification_report(y_val, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5U0g8_REdAG","outputId":"3fd6a3e5-4ac8-4344-d4e8-5fd8bbf3c6bd"},"outputs":[{"data":{"text/plain":["array(['Unrealistic Hope', 'Generalized Hope', 'Not Hope', ...,\n","       'Not Hope', 'Generalized Hope', 'Realistic Hope'], dtype=object)"]},"execution_count":115,"metadata":{},"output_type":"execute_result"}],"source":["y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_xLtZ2mEdAG"},"outputs":[],"source":["y_predtest1 = lr_classifier.predict(X_test1)\n","y_predtest2 = lr_classifier.predict(X_test2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PuTcsGh6EdAG","outputId":"22591f3e-2f8c-4c5e-9082-7a1f1158b4d6"},"outputs":[{"data":{"text/plain":["array(['Not Hope', 'Not Hope', 'Not Hope', ..., 'Not Hope', 'Not Hope',\n","       'Unrealistic Hope'], dtype=object)"]},"execution_count":117,"metadata":{},"output_type":"execute_result"}],"source":["y_predtest1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BNufS8x4EdAG","outputId":"c84f27a0-e696-42a8-a6f5-efae0fee5e65"},"outputs":[{"data":{"text/plain":["array(['Not Hope', 'Realistic Hope', 'Realistic Hope', ...,\n","       'Unrealistic Hope', 'Not Hope', 'Generalized Hope'], dtype=object)"]},"execution_count":118,"metadata":{},"output_type":"execute_result"}],"source":["y_predtest2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ly-dOlczEdAG","outputId":"174f38fd-589d-4f3d-b826-d5cf26e460d1"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["                  precision    recall  f1-score   support\n","\n","Generalized Hope       0.52      0.49      0.50       486\n","        Not Hope       0.82      0.63      0.72      1301\n","  Realistic Hope       0.26      0.43      0.32       202\n","Unrealistic Hope       0.26      0.53      0.35       193\n","\n","        accuracy                           0.57      2182\n","       macro avg       0.47      0.52      0.47      2182\n","    weighted avg       0.65      0.57      0.60      2182\n","\n"]}],"source":["from sklearn.svm import LinearSVC\n","lsvc_model = LinearSVC(C=1.0, class_weight='balanced')\n","lsvc_model.fit(X_train.tolist(), y_train.tolist())\n","y_pred_lsvc = lsvc_model.predict(X_val.tolist())\n","print( classification_report(y_val, y_pred_lsvc))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZ4Zu-XpEdAG"},"outputs":[],"source":["#Submission\n","# to create .csv file consisting of tweet ids and predicted labels with CTbert\n","pred_test = pd.DataFrame(data=y_predtest1, columns=['Prediction'])\n","submission_t2 = pd.DataFrame()\n","submission_t2['id'] = df_test_span['id']\n","submission_t2['category'] = pred_test\n","submission_t2.to_csv('algvec_lroversampl_span16th.csv', index = None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdiFQ8LaEdAG"},"outputs":[],"source":["#Submission\n","# to create .csv file consisting of tweet ids and predicted labels with CTbert\n","pred_test = pd.DataFrame(data=y_predtest1, columns=['Prediction'])\n","submission_t2 = pd.DataFrame()\n","submission_t2['id'] = df_test_Eng['id']\n","submission_t2['category'] = pred_test\n","submission_t2.to_csv('algvec_lroversampl_Eng16th.csv', index = None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Og17nAneEdAG"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}